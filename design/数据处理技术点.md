# 数据处理技术点详解

## 1. CSV数据解析策略

### 1.1 文件格式分析
基于对数据文件的深入分析，确定了统一的数据格式：

```python
# 文件结构定义
class CSVStructure:
    METADATA_ROW = 0           # 第1行：采样频率，数据点数等
    CONFIG_ROWS = [1, 2, 3]    # 第2-4行：传感器配置参数
    DATA_START_ROW = 4         # 第5行开始：实际传感器数据
    CHANNEL_COUNT = 10         # 总通道数：9个加速度+1个转速

    # 通道映射关系
    CHANNEL_MAP = {
        'A_X': 0, 'A_Y': 1, 'A_Z': 2,   # 传感器A三轴
        'B_X': 3, 'B_Y': 4, 'B_Z': 5,   # 传感器B三轴
        'C_X': 6, 'C_Y': 7, 'C_Z': 8,   # 传感器C三轴
        'SPEED': 9                        # 转速通道
    }
```

### 1.2 内存优化读取技术

#### 1.2.1 分块读取算法
```python
class ChunkReader:
    def __init__(self, chunk_size=10000):
        self.chunk_size = chunk_size

    def read_channel_chunk(self, filepath, channel_idx, start_row, end_row):
        """
        按块读取指定通道数据，避免内存溢出

        Args:
            filepath: CSV文件路径
            channel_idx: 通道索引（0-9）
            start_row: 开始行号
            end_row: 结束行号

        Returns:
            numpy.ndarray: 通道数据
        """
        chunks = []
        current_row = start_row

        while current_row < end_row:
            chunk_end = min(current_row + self.chunk_size, end_row)

            # 只读取指定列，跳过元数据行
            chunk_data = pd.read_csv(
                filepath,
                usecols=[channel_idx],
                skiprows=list(range(4)) + list(range(current_row + 4)),
                nrows=chunk_end - current_row,
                header=None
            )

            chunks.append(chunk_data.values.flatten())
            current_row = chunk_end

        return np.concatenate(chunks)
```

#### 1.2.2 懒加载机制
```python
class LazyDataLoader:
    def __init__(self, filepath):
        self.filepath = filepath
        self.metadata = None
        self.data_cache = {}

    @property
    def sampling_rate(self):
        """延迟加载元数据"""
        if self.metadata is None:
            self.metadata = self._load_metadata()
        return self.metadata['sampling_rate']

    def get_channel_data(self, sensor, axis, time_range=None):
        """
        懒加载指定通道数据

        Args:
            sensor: 传感器标识 ('A', 'B', 'C')
            axis: 轴向 ('X', 'Y', 'Z')
            time_range: 时间范围 (start_time, end_time)

        Returns:
            SignalData: 包装后的信号数据
        """
        cache_key = f"{sensor}_{axis}"

        if time_range:
            start_idx = int(time_range[0] * self.sampling_rate)
            end_idx = int(time_range[1] * self.sampling_rate)
            cache_key += f"_{start_idx}_{end_idx}"

        if cache_key not in self.data_cache:
            self.data_cache[cache_key] = self._load_channel_data(
                sensor, axis, time_range
            )

        return self.data_cache[cache_key]
```

### 1.3 数据验证与错误处理

#### 1.3.1 文件完整性验证
```python
class DataValidator:
    @staticmethod
    def validate_csv_structure(filepath):
        """验证CSV文件结构的完整性"""
        try:
            # 检查文件大小
            file_size = os.path.getsize(filepath)
            if file_size < 1024 * 1024:  # 小于1MB可能不完整
                raise ValueError("文件大小异常，可能数据不完整")

            # 验证行数
            with open(filepath, 'r') as f:
                line_count = sum(1 for line in f)

            expected_lines = 460805
            if line_count != expected_lines:
                raise ValueError(f"数据行数不匹配：期望{expected_lines}，实际{line_count}")

            # 验证元数据格式
            metadata = pd.read_csv(filepath, nrows=1, header=None)
            if metadata.shape[1] < 5:
                raise ValueError("元数据格式不正确")

            return True

        except Exception as e:
            logging.error(f"文件验证失败: {filepath}, 错误: {e}")
            return False

    @staticmethod
    def validate_data_range(data, sensor, axis):
        """验证数据数值范围的合理性"""
        # 加速度数据通常在合理范围内
        if sensor in ['A', 'B', 'C'] and axis in ['X', 'Y', 'Z']:
            max_val = np.max(np.abs(data))
            if max_val > 1000:  # 加速度异常大
                logging.warning(f"传感器{sensor}_{axis}数据异常: 最大值{max_val}")

        # 检查NaN值
        nan_count = np.isnan(data).sum()
        if nan_count > 0:
            logging.warning(f"数据包含{nan_count}个NaN值")

        return True
```

## 2. 多级缓存系统

### 2.1 缓存层次结构
```python
class HierarchicalCache:
    def __init__(self, config):
        self.l1_cache = {}          # 内存缓存 - 最常用数据
        self.l2_cache_dir = config.cache_dir  # 磁盘缓存 - 预处理结果
        self.max_memory_usage = 500 * 1024 * 1024  # 500MB内存限制

    def get(self, key):
        """三级缓存查找策略"""
        # L1: 内存缓存
        if key in self.l1_cache:
            return self.l1_cache[key]

        # L2: 磁盘缓存
        cache_file = os.path.join(self.l2_cache_dir, f"{key}.pkl")
        if os.path.exists(cache_file):
            data = pickle.load(open(cache_file, 'rb'))
            self._add_to_l1_cache(key, data)
            return data

        # L3: 原始数据加载
        return None

    def put(self, key, data):
        """智能缓存存储策略"""
        # 计算数据大小
        data_size = self._calculate_size(data)

        # 大数据直接存磁盘缓存
        if data_size > 50 * 1024 * 1024:  # 50MB
            self._save_to_disk(key, data)
        else:
            # 小数据存内存缓存
            self._add_to_l1_cache(key, data)
            # 同时备份到磁盘
            self._save_to_disk(key, data)
```

### 2.2 LRU内存管理
```python
from collections import OrderedDict
import threading

class LRUCache:
    def __init__(self, max_size_mb=200):
        self.max_size = max_size_mb * 1024 * 1024
        self.current_size = 0
        self.cache = OrderedDict()
        self.lock = threading.Lock()

    def get(self, key):
        with self.lock:
            if key in self.cache:
                # 移到最后（最近使用）
                self.cache.move_to_end(key)
                return self.cache[key]['data']
            return None

    def put(self, key, data):
        with self.lock:
            data_size = self._get_data_size(data)

            # 如果数据太大，直接拒绝缓存
            if data_size > self.max_size * 0.5:
                return False

            # 清理空间
            while (self.current_size + data_size > self.max_size
                   and len(self.cache) > 0):
                self._evict_lru()

            # 添加新数据
            self.cache[key] = {
                'data': data,
                'size': data_size,
                'timestamp': time.time()
            }
            self.current_size += data_size

            return True

    def _evict_lru(self):
        """淘汰最久未使用的数据"""
        if self.cache:
            key, value = self.cache.popitem(last=False)
            self.current_size -= value['size']
            logging.info(f"缓存淘汰: {key}, 释放{value['size']}字节")
```

## 3. 文件名解析与配置管理

### 3.1 文件名解析器
```python
import re
from dataclasses import dataclass

@dataclass
class FileConfig:
    drive_gear_state: str      # 主动轮状态
    driven_gear_state: str     # 从动轮状态
    torque: int               # 扭矩值
    speed: int                # 转速
    filepath: str             # 完整文件路径

class FilenameParser:
    """解析齿轮数据文件名的配置信息"""

    # 文件名正则表达式
    PATTERN = re.compile(
        r'主动轮\((.+?)\)-从动轮\((.+?)\)-(\d+)Nm-(\d+)r\.csv'
    )

    WEAR_STATE_MAP = {
        '正常': 'normal',
        '轻磨': 'light_wear',
        '重磨': 'heavy_wear'
    }

    @classmethod
    def parse(cls, filepath):
        """
        从文件路径解析配置信息

        Args:
            filepath: 文件路径

        Returns:
            FileConfig: 解析后的配置对象

        Raises:
            ValueError: 文件名格式不符合规范
        """
        filename = os.path.basename(filepath)
        match = cls.PATTERN.match(filename)

        if not match:
            raise ValueError(f"文件名格式不正确: {filename}")

        drive_state, driven_state, torque, speed = match.groups()

        return FileConfig(
            drive_gear_state=cls.WEAR_STATE_MAP.get(drive_state, drive_state),
            driven_gear_state=cls.WEAR_STATE_MAP.get(driven_state, driven_state),
            torque=int(torque),
            speed=int(speed),
            filepath=filepath
        )

    @classmethod
    def get_available_configs(cls, data_dir):
        """扫描目录获取所有可用的配置组合"""
        configs = []

        for filename in os.listdir(data_dir):
            if filename.endswith('.csv'):
                try:
                    config = cls.parse(os.path.join(data_dir, filename))
                    configs.append(config)
                except ValueError as e:
                    logging.warning(f"跳过无效文件: {filename}, {e}")

        return configs
```

### 3.2 配置驱动的数据访问
```python
class ConfigBasedDataLoader:
    def __init__(self, data_dir):
        self.data_dir = data_dir
        self.available_configs = FilenameParser.get_available_configs(data_dir)
        self.file_cache = {}

    def get_matching_files(self, drive_state=None, driven_state=None,
                          torque=None, speed=None):
        """根据条件筛选匹配的文件配置"""
        filtered = self.available_configs

        if drive_state:
            filtered = [c for c in filtered if c.drive_gear_state == drive_state]
        if driven_state:
            filtered = [c for c in filtered if c.driven_gear_state == driven_state]
        if torque:
            filtered = [c for c in filtered if c.torque == torque]
        if speed:
            filtered = [c for c in filtered if c.speed == speed]

        return filtered

    def load_data(self, config, sensor, axis, time_range=None):
        """根据配置加载数据"""
        cache_key = (config.filepath, sensor, axis, time_range)

        if cache_key not in self.file_cache:
            loader = LazyDataLoader(config.filepath)
            self.file_cache[cache_key] = loader.get_channel_data(
                sensor, axis, time_range
            )

        return self.file_cache[cache_key]
```

## 4. 数据预处理技术

### 4.1 数据清洗
```python
class DataCleaner:
    @staticmethod
    def remove_outliers(data, method='iqr', threshold=3):
        """异常值检测与处理"""
        if method == 'iqr':
            Q1 = np.percentile(data, 25)
            Q3 = np.percentile(data, 75)
            IQR = Q3 - Q1
            lower_bound = Q1 - threshold * IQR
            upper_bound = Q3 + threshold * IQR

            # 标记异常值但不删除，而是用边界值替代
            cleaned_data = np.copy(data)
            cleaned_data[data < lower_bound] = lower_bound
            cleaned_data[data > upper_bound] = upper_bound

        elif method == 'zscore':
            z_scores = np.abs(stats.zscore(data))
            cleaned_data = data[z_scores < threshold]

        return cleaned_data

    @staticmethod
    def interpolate_missing_values(data, method='linear'):
        """插值填补缺失值"""
        if np.any(np.isnan(data)):
            df = pd.Series(data)
            if method == 'linear':
                return df.interpolate(method='linear').values
            elif method == 'cubic':
                return df.interpolate(method='cubic').values
        return data
```

### 4.2 数据标准化
```python
class DataNormalizer:
    @staticmethod
    def normalize_amplitude(data, method='minmax'):
        """幅值标准化"""
        if method == 'minmax':
            return (data - np.min(data)) / (np.max(data) - np.min(data))
        elif method == 'zscore':
            return (data - np.mean(data)) / np.std(data)
        elif method == 'robust':
            median = np.median(data)
            mad = np.median(np.abs(data - median))
            return (data - median) / mad

    @staticmethod
    def detrend_signal(data, method='linear'):
        """去除趋势"""
        if method == 'linear':
            return signal.detrend(data, type='linear')
        elif method == 'constant':
            return signal.detrend(data, type='constant')
        return data
```

这些技术点确保了数据处理的高效性、准确性和可靠性，为后续的信号分析提供了坚实的基础。